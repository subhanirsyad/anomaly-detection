{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deteksi Anomali Aktivitas User (CLUE-style Swap UID + IsolationForest + SHAP)\n",
        "\n",
        "Notebook ini merapikan pipeline menjadi beberapa tahap yang jelas:\n",
        "\n",
        "1. **Konfigurasi & setup**\n",
        "2. **Membaca data JSON Lines (`.jsonl`) secara streaming**\n",
        "3. **Cleaning & validasi kolom penting (`uid`, `time`, `type`)**\n",
        "4. **EDA/Visualisasi ringkas (aktivitas per tahun/bulan + user unik)**\n",
        "5. **Ambil sampel tahun target + injeksi anomali sintetis (swap UID)**\n",
        "6. **Feature engineering harian (count + ratio + log)**\n",
        "7. **Modeling: IsolationForest + tuning threshold (F1)**\n",
        "8. **Explainability: SHAP (global importance + top fitur)**\n",
        "9. **Output: skor anomali & prediksi (level harian dan join ke event)**\n",
        "\n",
        "> Catatan: File data besar sebaiknya diproses **streaming** (tidak diload penuh ke memori).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 0) Setup: dependency & import\n",
        "# ============================================================\n",
        "import sys, subprocess, os, json, math\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "# (opsional) install shap kalau belum ada\n",
        "try:\n",
        "    import shap\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"shap\"])\n",
        "    import shap\n",
        "\n",
        "shap.initjs()\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "pd.set_option(\"display.width\", 120)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Konfigurasi\n",
        "\n",
        "Sesuaikan `DATA_PATH` ke lokasi file JSON Lines kamu.\n",
        "\n",
        "- Format input: **1 event per baris**, JSON object yang minimal punya:\n",
        "  - `uid`  : id user\n",
        "  - `time` : timestamp ISO (contoh: `\"2020-01-01T12:34:56Z\"`)\n",
        "  - `type` : tipe event (contoh: `\"login\"`, `\"file_accessed\"`, dll)\n",
        "\n",
        "Jika kolom kamu berbeda (mis. `event_type`), fungsi `normalize_record()` di bawah akan mencoba memetakan otomatis.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 1) Konfigurasi\n",
        "# ============================================================\n",
        "# Jika kamu di Google Colab dan file ada di Drive, aktifkan 2 baris ini:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "DATA_PATH = Path(\"/content/drive/MyDrive/Projek/clue.json\")  # ganti sesuai lokasi kamu\n",
        "\n",
        "YEARS_INTEREST = list(range(2017, 2024))  # 2017–2023\n",
        "TARGET_YEAR = 2020\n",
        "N_SAMPLE = 50_000\n",
        "\n",
        "# Injeksi anomali (swap UID)\n",
        "ANOM_N_PAIRS = 20\n",
        "MIN_EVENTS_PER_USER = 80\n",
        "MIN_OVERLAP_DAYS = 2\n",
        "PREFER_DISSIMILAR = True\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "assert TARGET_YEAR in range(min(YEARS_INTEREST), max(YEARS_INTEREST)+2), \"TARGET_YEAR di luar rentang wajar.\"\n",
        "print(\"DATA_PATH:\", DATA_PATH)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Membaca data (streaming) + cleaning\n",
        "\n",
        "Di sini kita definisikan helper untuk:\n",
        "- membaca file JSON Lines baris demi baris\n",
        "- normalisasi field (`uid`, `time`, `type`)\n",
        "- parsing timestamp\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2) Helpers: iterasi jsonl & normalisasi record\n",
        "# ============================================================\n",
        "def iter_jsonl(path: Path):\n",
        "    \"\"\"Yield dict per baris dari file JSON Lines (.jsonl).\"\"\"\n",
        "    path = Path(path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"File tidak ditemukan: {path}\\n\"\n",
        "            \"Pastikan DATA_PATH sudah benar, atau upload file ke runtime.\"\n",
        "        )\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                yield json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "def parse_iso_datetime(ts: str):\n",
        "    \"\"\"Parse timestamp ISO cepat (mendukung akhiran 'Z'). Return datetime atau None.\"\"\"\n",
        "    if not ts:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def normalize_record(rec: dict):\n",
        "    \"\"\"Mapping field berbeda ke skema standar: uid, time, type.\"\"\"\n",
        "    uid = rec.get(\"uid\", rec.get(\"user_id\", rec.get(\"userid\")))\n",
        "    ts  = rec.get(\"time\", rec.get(\"timestamp\", rec.get(\"ts\")))\n",
        "    typ = rec.get(\"type\", rec.get(\"event_type\", rec.get(\"action\")))\n",
        "    if uid is None or ts is None or typ is None:\n",
        "        return None\n",
        "    dt = parse_iso_datetime(ts)\n",
        "    if dt is None:\n",
        "        return None\n",
        "    return {\n",
        "        \"uid\": str(uid),\n",
        "        \"time\": dt,          # datetime (timezone-aware jika input ISO +00:00)\n",
        "        \"type\": str(typ)\n",
        "    }\n",
        "\n",
        "# quick sanity: hitung 5 record valid pertama\n",
        "tmp = []\n",
        "for rec in iter_jsonl(DATA_PATH):\n",
        "    norm = normalize_record(rec)\n",
        "    if norm is not None:\n",
        "        tmp.append(norm)\n",
        "    if len(tmp) >= 5:\n",
        "        break\n",
        "display(pd.DataFrame(tmp))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) EDA Streaming: aktivitas per tahun/bulan + user unik\n",
        "\n",
        "Menghitung:\n",
        "- total event per tahun (2017–2023)\n",
        "- user unik per tahun\n",
        "- total event per (tahun, bulan) untuk visualisasi tren bulanan\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3) EDA Streaming\n",
        "# ============================================================\n",
        "def compute_activity_stats(path: Path, years_interest):\n",
        "    activity_per_year = defaultdict(int)\n",
        "    activity_per_year_month = defaultdict(int)  # (year, month) -> count\n",
        "    userset_per_year = {y: set() for y in years_interest}\n",
        "\n",
        "    for i, rec in enumerate(iter_jsonl(path), start=1):\n",
        "        norm = normalize_record(rec)\n",
        "        if norm is None:\n",
        "            continue\n",
        "        dt = norm[\"time\"]\n",
        "        y = dt.year\n",
        "        if y in years_interest:\n",
        "            activity_per_year[y] += 1\n",
        "            activity_per_year_month[(y, dt.month)] += 1\n",
        "            userset_per_year[y].add(norm[\"uid\"])\n",
        "\n",
        "        if i % 1_000_000 == 0:\n",
        "            print(f\"Processed {i:,} lines ...\")\n",
        "\n",
        "    return activity_per_year, activity_per_year_month, userset_per_year\n",
        "\n",
        "activity_per_year, activity_per_year_month, userset_per_year = compute_activity_stats(DATA_PATH, YEARS_INTEREST)\n",
        "\n",
        "years = YEARS_INTEREST\n",
        "totals = [activity_per_year.get(y, 0) for y in years]\n",
        "user_counts = [len(userset_per_year.get(y, set())) for y in years]\n",
        "\n",
        "# Plot: total event per tahun\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.bar(years, totals)\n",
        "plt.title(\"Total Aktivitas per Tahun (2017–2023)\")\n",
        "plt.xlabel(\"Tahun\"); plt.ylabel(\"Jumlah event\")\n",
        "for y, v in zip(years, totals):\n",
        "    plt.text(y, v, f\"{v:,}\", ha=\"center\", va=\"bottom\", fontsize=8, rotation=90)\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# Plot: user unik per tahun\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.bar(years, user_counts)\n",
        "plt.title(\"Jumlah User Unik per Tahun (2017–2023)\")\n",
        "plt.xlabel(\"Tahun\"); plt.ylabel(\"Jumlah user unik\")\n",
        "for y, v in zip(years, user_counts):\n",
        "    plt.text(y, v, str(v), ha=\"center\", va=\"bottom\", fontsize=8)\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# Tambahan: aktivitas per bulan per tahun\n",
        "df_month = pd.DataFrame(\n",
        "    [{\"year\": y, \"month\": m, \"count\": c} for (y,m), c in activity_per_year_month.items()]\n",
        ")\n",
        "if not df_month.empty:\n",
        "    pivot_month = df_month.pivot_table(index=\"month\", columns=\"year\", values=\"count\", fill_value=0).sort_index()\n",
        "    plt.figure(figsize=(10,4))\n",
        "    for y in pivot_month.columns:\n",
        "        plt.plot(pivot_month.index, pivot_month[y], marker=\"o\", label=str(y))\n",
        "    plt.title(\"Aktivitas per Bulan (by Tahun)\")\n",
        "    plt.xlabel(\"Bulan\"); plt.ylabel(\"Jumlah event\")\n",
        "    plt.xticks(range(1,13))\n",
        "    plt.grid(True); plt.legend(ncols=4, fontsize=8)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# Proporsi kontribusi per tahun\n",
        "overall = sum(totals) if sum(totals) > 0 else 1\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.pie([t/overall for t in totals], labels=years, autopct=\"%1.1f%%\", startangle=140)\n",
        "plt.title(\"Proporsi Aktivitas per Tahun\")\n",
        "plt.tight_layout(); plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Ambil sampel tahun target + basic cleaning\n",
        "\n",
        "Agar modeling cepat, kita ambil sampel `N_SAMPLE` event untuk `TARGET_YEAR`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 4) Load sampel TARGET_YEAR\n",
        "# ============================================================\n",
        "def load_year_sample(path: Path, year: int, n_sample: int):\n",
        "    rows = []\n",
        "    for rec in iter_jsonl(path):\n",
        "        norm = normalize_record(rec)\n",
        "        if norm is None:\n",
        "            continue\n",
        "        if norm[\"time\"].year == year:\n",
        "            rows.append(norm)\n",
        "            if len(rows) >= n_sample:\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        raise ValueError(f\"Tidak ada data untuk year={year}. Cek DATA_PATH / format timestamp.\")\n",
        "    df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True)\n",
        "    df[\"date\"] = df[\"time\"].dt.date\n",
        "    df[\"label\"] = 1  # normal = +1\n",
        "    return df\n",
        "\n",
        "df_year = load_year_sample(DATA_PATH, TARGET_YEAR, N_SAMPLE)\n",
        "\n",
        "print(\"Preview data NORMAL (+1) sebelum injeksi anomaly:\")\n",
        "display(df_year.head())\n",
        "print(\"Distribusi type (top 10):\")\n",
        "display(df_year[\"type\"].value_counts().head(10))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Injeksi anomali sintetis: swap UID (CLUE-style)\n",
        "\n",
        "Ide:\n",
        "- pilih pasangan user aktif\n",
        "- cari irisan hari (overlap) aktivitas\n",
        "- tentukan `ts` (median overlap)\n",
        "- **setelah `ts`**, tukar `uid` untuk kedua user → label event menjadi **-1**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 5) Injeksi anomaly: swap UID\n",
        "# ============================================================\n",
        "def create_anomalies_by_user_swap(\n",
        "    df_in: pd.DataFrame,\n",
        "    n_pairs: int = 15,\n",
        "    random_state: int = 42,\n",
        "    min_events_per_user: int = 100,\n",
        "    min_overlap_days: int = 2,\n",
        "    prefer_dissimilar: bool = True,\n",
        "    max_jaccard: float = 0.6\n",
        "):\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    df = df_in.copy()\n",
        "\n",
        "    # kandidat user aktif\n",
        "    counts = df[\"uid\"].value_counts()\n",
        "    candidates = counts[counts >= min_events_per_user].index.tolist()\n",
        "    if len(candidates) < 2:\n",
        "        print(\"User aktif terlalu sedikit. Turunkan min_events_per_user.\")\n",
        "        return df, []\n",
        "\n",
        "    rng.shuffle(candidates)\n",
        "\n",
        "    # precompute set event types & hari aktif per user\n",
        "    user_eventset = {}\n",
        "    user_days = {}\n",
        "    for u in candidates:\n",
        "        sub = df[df[\"uid\"] == u]\n",
        "        user_eventset[u] = set(sub[\"type\"].unique())\n",
        "        user_days[u] = set(sub[\"date\"].unique())\n",
        "\n",
        "    def jaccard(a, b):\n",
        "        if not a and not b:\n",
        "            return 1.0\n",
        "        inter = len(a & b)\n",
        "        union = len(a | b)\n",
        "        return inter / union if union > 0 else 0.0\n",
        "\n",
        "    anomaly_idx = set()\n",
        "    pairs = []\n",
        "\n",
        "    i = 0\n",
        "    while i + 1 < len(candidates) and len(pairs) < n_pairs:\n",
        "        u1, u2 = candidates[i], candidates[i + 1]\n",
        "        i += 2\n",
        "\n",
        "        overlap = np.intersect1d(list(user_days[u1]), list(user_days[u2]))\n",
        "        if len(overlap) < min_overlap_days:\n",
        "            continue\n",
        "\n",
        "        sim = jaccard(user_eventset[u1], user_eventset[u2])\n",
        "        if prefer_dissimilar and sim > max_jaccard:\n",
        "            continue\n",
        "\n",
        "        ts = np.sort(overlap)[len(overlap) // 2]  # median overlap day\n",
        "        mask1 = (df[\"uid\"] == u1) & (df[\"date\"] >= ts)\n",
        "        mask2 = (df[\"uid\"] == u2) & (df[\"date\"] >= ts)\n",
        "        if mask1.sum() == 0 or mask2.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        anomaly_idx.update(df[mask1].index.tolist())\n",
        "        anomaly_idx.update(df[mask2].index.tolist())\n",
        "\n",
        "        # swap uid (setelah ts)\n",
        "        df.loc[mask1, \"uid\"] = u2\n",
        "        df.loc[mask2, \"uid\"] = u1\n",
        "\n",
        "        pairs.append((u1, u2, ts, sim))\n",
        "\n",
        "    df.loc[list(anomaly_idx), \"label\"] = -1\n",
        "\n",
        "    print(f\"Total pasangan swap: {len(pairs)}\")\n",
        "    if pairs:\n",
        "        print(\"Contoh pasangan (u1, u2, ts, jaccard):\", pairs[:5])\n",
        "    print(\"Distribusi label (setelah injeksi):\")\n",
        "    print(df[\"label\"].value_counts())\n",
        "    return df, pairs\n",
        "\n",
        "df_anom, swap_pairs = create_anomalies_by_user_swap(\n",
        "    df_year,\n",
        "    n_pairs=ANOM_N_PAIRS,\n",
        "    random_state=RANDOM_STATE,\n",
        "    min_events_per_user=MIN_EVENTS_PER_USER,\n",
        "    min_overlap_days=MIN_OVERLAP_DAYS,\n",
        "    prefer_dissimilar=PREFER_DISSIMILAR,\n",
        ")\n",
        "\n",
        "print(\"Preview data SETELAH injeksi anomaly:\")\n",
        "display(df_anom.sample(10, random_state=RANDOM_STATE).sort_values(\"time\")[[\"time\",\"uid\",\"type\",\"label\"]])\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Vektorisasi harian: (uid, date) × event_type (count)\n",
        "\n",
        "- Fitur harian = jumlah event per `type`.\n",
        "- Label harian = **-1** jika ada minimal 1 event anomaly pada hari tersebut, selain itu **+1**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 6) Vektorisasi harian\n",
        "# ============================================================\n",
        "def build_daily_vectors(df_in: pd.DataFrame):\n",
        "    d = df_in.copy()\n",
        "\n",
        "    X_counts = d.pivot_table(\n",
        "        index=[\"uid\", \"date\"],\n",
        "        columns=\"type\",\n",
        "        values=\"time\",\n",
        "        aggfunc=\"count\",\n",
        "        fill_value=0\n",
        "    ).sort_index()\n",
        "\n",
        "    # label harian: min() karena -1 < +1 → kalau ada -1, hasil -1\n",
        "    y_day = d.groupby([\"uid\",\"date\"])[\"label\"].min().reindex(X_counts.index).astype(int)\n",
        "\n",
        "    return X_counts, y_day\n",
        "\n",
        "X_counts, y_day = build_daily_vectors(df_anom)\n",
        "\n",
        "print(\"Shape vektor harian (counts-only):\", X_counts.shape)\n",
        "print(\"Distribusi label harian (1 normal, -1 anomaly):\")\n",
        "print(y_day.value_counts())\n",
        "display(X_counts.head())\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Feature Engineering (ringan tapi efektif)\n",
        "\n",
        "Tambahan fitur:\n",
        "- `total_events`\n",
        "- `unique_event_types`\n",
        "- log transform untuk count: `log1p(count)`\n",
        "- ratio per tipe event: `count / total_events`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 7) Feature engineering\n",
        "# ============================================================\n",
        "def make_aug_features(X_counts: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X_counts.copy()\n",
        "\n",
        "    extra = pd.DataFrame(index=X.index)\n",
        "    extra[\"total_events\"] = X.sum(axis=1)\n",
        "    extra[\"unique_event_types\"] = (X > 0).sum(axis=1)\n",
        "\n",
        "    # log transform untuk kurangi skew\n",
        "    X_log = np.log1p(X)\n",
        "\n",
        "    # proporsi tiap event type\n",
        "    denom = extra[\"total_events\"].replace(0, np.nan)\n",
        "    ratio = X.div(denom, axis=0).fillna(0).add_prefix(\"ratio_\")\n",
        "\n",
        "    X_aug = pd.concat([X_log, extra, ratio], axis=1)\n",
        "    return X_aug\n",
        "\n",
        "X_aug = make_aug_features(X_counts)\n",
        "X_train = X_aug[y_day == 1]          # fit di normal saja\n",
        "y_true_full = y_day.copy()           # 1 normal, -1 anomaly\n",
        "y_true_bin = (y_true_full == -1).astype(int)  # 1=anomaly, 0=normal\n",
        "\n",
        "print(\"X_aug shape:\", X_aug.shape)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Modeling: IsolationForest + tuning threshold (F1)\n",
        "\n",
        "Strategi:\n",
        "- Fit IsolationForest di data **normal**.\n",
        "- Skor anomali untuk semua data: `scores = -decision_function(...)` (makin besar = makin anomali).\n",
        "- Sweep threshold berbasis percentile skor untuk memaksimalkan **F1**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 8) Modeling + tuning threshold\n",
        "# ============================================================\n",
        "def plot_confusion_matrix(cm, title=\"Confusion Matrix\", classes=(\"Normal (0)\", \"Anomaly (1)\")):\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    im = ax.imshow(cm, interpolation=\"nearest\")\n",
        "    ax.set_title(title)\n",
        "    plt.colorbar(im, ax=ax)\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    ax.set_xticks(tick_marks); ax.set_xticklabels(classes, rotation=45)\n",
        "    ax.set_yticks(tick_marks); ax.set_yticklabels(classes)\n",
        "\n",
        "    thresh = cm.max() / 2.0 if cm.size else 0\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], \"d\"),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    ax.set_ylabel(\"True label\")\n",
        "    ax.set_xlabel(\"Predicted label\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_tune_iforest(X_aug: pd.DataFrame, X_train: pd.DataFrame, y_true_bin: np.ndarray, random_state: int = 42):\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [300, 600],\n",
        "        \"max_samples\": [0.6, 0.8, 1.0],\n",
        "        \"contamination\": [0.05, 0.1],  # baseline; threshold sebenarnya dari sweep\n",
        "    }\n",
        "    percentiles = np.linspace(50, 99, 100)\n",
        "\n",
        "    best = {\"f1\": -1, \"params\": None, \"threshold\": None, \"pred\": None, \"model\": None, \"scores\": None}\n",
        "\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        model = IsolationForest(random_state=random_state, n_jobs=-1, **params)\n",
        "        model.fit(X_train)\n",
        "\n",
        "        scores = -model.decision_function(X_aug)  # skor besar = lebih aneh\n",
        "\n",
        "        for p in percentiles:\n",
        "            t = np.percentile(scores, p)\n",
        "            y_pred_bin = (scores > t).astype(int)\n",
        "            f1 = f1_score(y_true_bin, y_pred_bin)\n",
        "\n",
        "            if f1 > best[\"f1\"]:\n",
        "                best.update({\"f1\": f1, \"params\": params, \"threshold\": t,\n",
        "                             \"pred\": y_pred_bin, \"model\": model, \"scores\": scores})\n",
        "\n",
        "    return best\n",
        "\n",
        "best = train_tune_iforest(X_aug, X_train, y_true_bin, random_state=RANDOM_STATE)\n",
        "\n",
        "print(\"=== Best IsolationForest (tuned) ===\")\n",
        "print(\"Params    :\", best[\"params\"])\n",
        "print(\"Threshold :\", best[\"threshold\"])\n",
        "print(\"Best F1   :\", best[\"f1\"])\n",
        "\n",
        "cm = confusion_matrix(y_true_bin, best[\"pred\"])\n",
        "plot_confusion_matrix(cm, \"Confusion Matrix - IsolationForest (tuned)\")\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_true_bin, best[\"pred\"], digits=4))\n",
        "\n",
        "if best[\"f1\"] < 0.70:\n",
        "    print(\"\\n[PERINGATAN] F1 < 0.70 di dataset ini.\")\n",
        "    print(\"Tips:\")\n",
        "    print(\"- Tambah ANOM_N_PAIRS (lebih banyak injeksi anomali)\")\n",
        "    print(\"- Turunkan MIN_EVENTS_PER_USER (lebih banyak kandidat user)\")\n",
        "    print(\"- longgarkan max_jaccard (kalau user terlalu mirip)\")\n",
        "    print(\"- Tambah fitur (mis. rolling mean/STD per user)\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Explainability: SHAP (global importance + top fitur)\n",
        "\n",
        "SHAP untuk IsolationForest berbasis tree bisa cukup berat, jadi kita sampling baris `X_aug`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 9) SHAP\n",
        "# ============================================================\n",
        "n_sample_shap = min(2000, X_aug.shape[0])\n",
        "rng = np.random.RandomState(RANDOM_STATE)\n",
        "idx = rng.choice(np.arange(X_aug.shape[0]), size=n_sample_shap, replace=False)\n",
        "X_shap = X_aug.iloc[idx]\n",
        "\n",
        "explainer = shap.TreeExplainer(best[\"model\"])\n",
        "shap_values = explainer.shap_values(X_shap)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "shap.summary_plot(shap_values, features=X_shap, feature_names=X_shap.columns, show=False)\n",
        "plt.title(\"SHAP Summary Plot - IsolationForest (tuned)\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# Global importance: mean(|SHAP|)\n",
        "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
        "imp = (pd.DataFrame({\"feature\": X_shap.columns, \"mean_abs_shap\": mean_abs_shap})\n",
        "       .sort_values(\"mean_abs_shap\", ascending=False)\n",
        "       .reset_index(drop=True))\n",
        "\n",
        "display(imp.head(20))\n",
        "\n",
        "top3 = imp.head(3)\n",
        "print(\"\\n=== 3 Fitur Paling Berpengaruh (mean |SHAP|) ===\")\n",
        "for i, row in top3.iterrows():\n",
        "    print(f\"{i+1}. {row['feature']} (mean|SHAP|={row['mean_abs_shap']:.6f})\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Output: skor anomali & prediksi\n",
        "\n",
        "- Level harian: gabungkan `scores` + prediksi\n",
        "- Join ke level event (`df_anom`) agar tiap event punya `anomaly_score_day` & `pred_day`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 10) Output + join\n",
        "# ============================================================\n",
        "scores_full = best[\"scores\"]\n",
        "pred_full = best[\"pred\"]\n",
        "\n",
        "df_daily = X_aug.copy()\n",
        "df_daily[\"anomaly_score\"] = scores_full\n",
        "df_daily[\"pred_bin\"] = pred_full           # 1=pred anomaly, 0=pred normal\n",
        "df_daily[\"true_bin\"] = y_true_bin.values   # 1=gt anomaly, 0=gt normal\n",
        "\n",
        "df_daily = df_daily.reset_index()  # uid, date jadi kolom\n",
        "\n",
        "# join ke event-level\n",
        "df_out = df_anom.merge(\n",
        "    df_daily[[\"uid\",\"date\",\"anomaly_score\",\"pred_bin\",\"true_bin\"]],\n",
        "    on=[\"uid\",\"date\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "print(\"Preview output event-level:\")\n",
        "display(df_out.head())\n",
        "\n",
        "# Opsional: simpan output (aktifkan jika perlu)\n",
        "# out_path = Path(\"output_with_scores.parquet\")\n",
        "# df_out.to_parquet(out_path, index=False)\n",
        "# print(\"Saved:\", out_path.resolve())\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) (Opsional) Visualisasi 1 pohon IsolationForest\n",
        "\n",
        "Berguna untuk sekilas melihat struktur split, tapi **bukan** interpretasi utama (lebih baik pakai SHAP).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 11) Visualisasi 1 pohon dari IsolationForest\n",
        "# ============================================================\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "one_tree = best[\"model\"].estimators_[0]\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(\n",
        "    one_tree,\n",
        "    feature_names=X_aug.columns,\n",
        "    max_depth=3,\n",
        "    filled=True,\n",
        "    fontsize=6\n",
        ")\n",
        "plt.title(\"Salah satu pohon IsolationForest (max_depth=3)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}